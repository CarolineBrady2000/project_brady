---
title: "Acquire data"
date: "`r Sys.Date()`"
bibliography: references.bib
biblio-style: apalike
---

## About

This section provides an overview of the process of acquiring data for exploratory analysis. It is important for this process to be communicable so the research is reproducible and individuals can build on these methods in the future.

### Description

The aim of this script is to show how we captured Twitter data to explore the prevalence and popularity of linguistic biases.

## Setup

The packages needed for acquiring data are shown in the code below. For example, rtweets helps us get twitter data in real time, as well as the geographic location of the tweets, and tidyverse includes many functions necessary for data analysis [@6.contr].

```{r load-important-packages}
library(rtweet)
library(tidyverse)
library(purrr)
```

Now we load our student token from Twitter into R so we are able to access and acquire Twitter data.

```{r load-student-token-for-Twitter-data}
student_token <- read_rds("student_token.rds") # path to the student_token.rds file
```

## Run

This section provides an overview of exactly how the data was acquired and organized. First, we create a vector of terms that reflect linguistic biases, and call it biased_terms.

```{r create-vector-of-biased-terms}
biased_terms <- c("no idea what he's saying", "no idea what she's saying", "weird accent", "thick accent", "bad accent", "native speaker", "bad english", "black english", "can't understand him", "can't understand her", "slang", "non-native speaker", "hard to understand", "language barrier", "barely speaks english", "Does have an accent") 
```

Now we create the term_search custom function so we can search for our biased terms in Twitter data.

```{r create-term_search-function}
term_search <- 
  function(search_term, n = 100) {
    # Function: 
    # Search recent tweets for specific term
    
    library(rtweet) # to search Twitter API
    
    tweets <- 
      search_tweets(q = sQuote(search_term), # query term (from search_term)
                    n = n, # number of desired tweets (from n)
                    include_rts = FALSE, # no retweets
                    geocode = lookup_coords("usa"), # only from US
                    token = student_token) %>%  # token for authentication
      lat_lng() %>% # extract the geocoordinates where available
      mutate(search_term = search_term) # add search_term value to the data frame
    return(tweets) # return the results
  }
```

Now we collect the twitter data with our custom term_search function to find a large sample of 500 tweets containing our biased terms.

```{r collect-500-tweets-and-count-frequency, eval=FALSE}
results <- 
  biased_terms %>% # terms to search
  map(term_search, n = 500) %>% # apply the function to each term, retrieve 500 tweets
  do_call_rbind() %>% # join the results by rows
  return() # return the results

results %>% 
  count(search_term, sort = TRUE) #count the number of tweets collected for each search term
```

We want to keep our first sample of tweets, so this code chunk makes sure we don't get a new sample of tweets every time the code is run. 
```{r conduct-the-search}
conduct_search <- FALSE # set to TRUE to conduct a new search

if(conduct_search) {
  
  cat("Conducting new search. \n")
  
  biased_terms <- c("no idea what he's saying", "no idea what she's saying", "weird accent", "thick accent", "bad accent", "native speaker", "bad english", "black english", "can't understand him", "can't understand her", "slang", "non-native speaker", "hard to understand", "language barrier", "barely speaks english", "Does have an accent")

results <- 
  biased_terms %>% # terms to search
  map(term_search, n = 500) %>% # apply the function to each term, retrieve 500 tweets
  do_call_rbind() %>% # join the results by rows
  return() # return the results
save_as_csv(file_name = "../data/original/bias_terms.csv")
  cat("Search results saved to disk. \n")

results %>% 
  count(search_term, sort = TRUE) #count the number of tweets collected for each search term 
  
} else {
  cat("Keeping previous search results. \n")
}

```

## Finalize

Now that we collected our data from twitter, we want to save it to a csv file in our directory. Our results showed 4542 observations with 93 variables, and this data will be curated into a useful table in the next step.

```{r save-as-csv-file, eval=FALSE}
save_as_csv(results, file_name = "../data/original/bias_terms.csv") #save the data results as a csv file 
```

### Log
The output of this code shows us a random sample of tweets containing the biased terms that hint at discriminatory language in tweets. Then the counts of each biased term will appear in a table, and we have another large table with 93 variables describing the tweets in which the biased terms appear.

```{r cleanup, echo=FALSE}
rm(list = ls()) # clean working environment
```

## References
