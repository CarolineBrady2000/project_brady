---
title: "Acquire data"
date: "`r Sys.Date()`"
bibliography: references.bib
biblio-style: apalike
---

## About
This section provides an overview of the process of acquiring data for exploratory analysis. It is important for this process to be communicable so the research is reproducible and individuals can build on these methods in the future. 

### Description
The aim of this script is to show how we captured Twitter data and United States Census data to explore the prevalence of linguistic biases.

## Setup
The three packages needed for acquiring data are shown in the code below. rtweets helps us get twitter data in real time, as well as the geographic location of the tweets. tidyverse includes many functions necessary for data analysis, and tidycensus includes data from the US census, including education levels of different regions of the United States. 

```{r}
library(rtweet)
library(tidyverse)
library(tidycensus)
library(readxl)
```

## Twitter Data 
```{r}
student_token <- create_token(
  app = app_name,
  consumer_key = api_key,
  consumer_secret = api_secret_key
)

saveRDS(student_token, file = "~/Desktop/student_token.rds")

```

## Run
<!-- Steps involved in acquiring and organizing the original data -->

```{r}
biased_terms <- c("no idea what he's saying", "no idea what she's saying", "weird accent", "thick accent", "bad accent", "native speaker", "bad english", "black english", "can't understand him", "can't understand her", "slang", "non-native speaker", "hard to understand", "counldn't understand a word he said", "couldn't understand a word she said", "language barrier", "barely speaks english", "Does have an accent")
```


```{r}
tweets <-
  search_tweets(q = biased_terms) # query terms showing linguistic biases
      n = 100 # number of desired tweets
      include_rts = FALSE # no retweets
      geocode = lookup_coords("usa") # only from US
      token = (R1BoZsc4rDk4Cgqm7cEJ6f7NF) %>%  # token for authentication
  lat_lng() # extract the geo-coordinates where available
```

```{r}
results <- 
  bias_terms %>% # terms to search
  map(term_search, n = 100) %>% # apply the function to each term of 100 tweets
  do_call_rbind() %>% # join the results by rows
  return() # return the results as a table 
```

```{r}
results %>%
    count(search_term, sort = TRUE) # counts the search terms in a table 
```

```{r}
results <- 
  bias_terms %>% # terms to search
  map(term_search, n = 500) %>% # apply the function to each term, retrieve 500 tweets (if available)
  do_call_rbind() %>% # join the results by rows
  return() # return the results
```

```{r}
states_map <- map_data("state")  

p <- ggplot() + geom_polygon(data = states_map, aes(x = long, y = lat, group = group),
    fill = "grey", color = "black") + labs(title = "Biased Tweets in the USA", subtitle = "Biased Language")

p + geom_point(data = results, aes(x = lng, y = lat, group = 1, color = search_term),
    alpha = 1/2, size = 1.5)
```

## Census Data 
This step in the process is determined by the first step. We will analyze the states with the highest density of biased tweets, then analyze demographic information about these states from US census data. 

```{r}
states_of_interest <- data.frame(State = c("Alabama", "Kentucky", "Missouri", "South Carolina", "Texas", "West Virginia"),
                                 Abb = c("AL", "KY", "MO", "SC", "TX", "WV"))
# enter whichever states have the mosted biased tweets from part 1 
```



```{r}
library(here)
census_edu <- as_tibble(read.csv(here("Census Data", "ACS_10_5YR_S1501_with_ann.csv"), header = T))

head(census_edu)
```

```{r}
View(census_edu)

census_edu_clean <- census_edu %>%
  select(County = GEO.display.label, PercentBachelorOrHigher = HC01_EST_VC05)
```

## Finalize 
```{r} 
save_as_csv(results, file_name = "../data/original/twitter/bias_terms.csv") #save the data results as a csv file 
```

### Log

The output of this code shows us a random sample of 100 tweets containing the biased terms that hint at discriminatory language in tweets. Then the counts of each biased term will appear in a table, and a map of the United States will show the origin of these tweets. This map will help us visualize where the biased tweets are concentrated in the United States. 

### Session

<details><summary>View session information</summary>

```{r, child="_session-info.Rmd"}
```

</details>

```{r cleanup, echo=FALSE}
rm(list = ls()) # clean working environment
```

## References
